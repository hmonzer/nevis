# =============================================================================
# Nevis API Configuration
# =============================================================================
### Copy this file to .env and modify values as needed.
### Environment variables use double underscore (__) as delimiter for nested values.
### All values shown are defaults - uncomment and modify to override.

# =============================================================================
# Application
# =============================================================================
APP_NAME="Nevis API"
APP_VERSION=1.0.0

# =============================================================================
# Database
# =============================================================================
DATABASE_URL=postgresql+asyncpg://localhost/nevis

# =============================================================================
# Search Settings
# =============================================================================
### General search pagination
SEARCH__DEFAULT_TOP_K=3
SEARCH__MAX_TOP_K=100

# =============================================================================
# Client Search (pg_trgm trigram similarity)
# =============================================================================
### Trigram similarity scores range from 0 to 1.
### Lower threshold = more lenient matching, higher = stricter.
### Default 0.32 works well for partial name matches.
CLIENT_SEARCH__TRGM_THRESHOLD=0.32

# =============================================================================
# Document Chunk Search (hybrid vector + keyword)
# =============================================================================
### Vector similarity threshold (cosine similarity, range: -1 to 1)
### Lower values return more results but with less precision.
CHUNK_SEARCH__VECTOR_SIMILARITY_THRESHOLD=0.3

### Retrieval multipliers control candidate pool size before ranking.
### With reranking: fetch top_k * multiplier candidates, then rerank to top_k.
CHUNK_SEARCH__RETRIEVAL_MULTIPLIER_WITH_RERANK=3
CHUNK_SEARCH__RETRIEVAL_MULTIPLIER_NO_RERANK=2

# =============================================================================
# Document Search
# =============================================================================
### How many chunks to retrieve per requested document.
### Higher values improve document ranking accuracy but increase latency.
DOCUMENT_SEARCH__CHUNK_RETRIEVAL_MULTIPLIER=5

# =============================================================================
# Reranker (CrossEncoder)
# =============================================================================
### CrossEncoder model for reranking search results.
RERANKER__MODEL_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2

### Score threshold for filtering reranked results.
### CrossEncoder logits typically range from -12 to +12.
### Positive = relevant, negative = irrelevant, 0.0 = 50% probability.
### Default 2.0 keeps results with ~88% relevance probability.
RERANKER__SCORE_THRESHOLD=2.0

# =============================================================================
# Reciprocal Rank Fusion (RRF)
# =============================================================================
### Smoothing constant that reduces the impact of high rankings.
### Default 60 is the standard value from the original RRF paper.
RRF__K=60

# =============================================================================
# Embedding Model
# =============================================================================
### SentenceTransformer model for generating embeddings.
### Must be compatible with the chunking size (chunk size <= model's max_seq_length).
EMBEDDING__MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# Chunking Settings
# =============================================================================
### Text chunking for document processing (values in tokens).
### IMPORTANT: chunk size must not exceed the embedding model's max_seq_length.
### For all-MiniLM-L6-v2, max_seq_length is 256 tokens.
CHUNKING__SIZE=256
CHUNKING__OVERLAP=25

# =============================================================================
# Summarization
# =============================================================================
### Enable/disable document summarization during ingestion.
SUMMARIZATION__ENABLED=false

### LLM provider for summarization: "claude" or "gemini"
SUMMARIZATION__PROVIDER=claude

### Target word count for generated summaries.
SUMMARIZATION__MAX_WORDS=100

### Maximum tokens for LLM response.
SUMMARIZATION__MAX_TOKENS=200

# =============================================================================
# LLM Settings
# =============================================================================
### Claude model for summarization (requires ANTHROPIC_API_KEY).
LLM__CLAUDE_MODEL=claude-sonnet-4-20250514

### Gemini model for summarization (requires GOOGLE_API_KEY).
LLM__GEMINI_MODEL=models/gemini-flash-latest

### API Keys (required if using respective provider for summarization)
#LLM__ANTHROPIC_API_KEY=<your-anthropic-api-key>
#LLM__GOOGLE_API_KEY=<your-google-api-key>

# =============================================================================
# S3 Storage
# =============================================================================
### S3 bucket for storing document content.
S3__BUCKET_NAME=nevis-documents

# S3 endpoint URL (set for LocalStack or S3-compatible services, leave empty for AWS).
S3__ENDPOINT_URL=http://localhost:4566

# Pattern for S3 document keys. Placeholders: {client_id}, {document_id}
S3__DOCUMENT_KEY_PATTERN=clients/{client_id}/documents/{document_id}.txt

# =============================================================================
# AWS Credentials
# =============================================================================
AWS__REGION=eu-west-1
AWS__ACCESS_KEY_ID=your-access-key-id
AWS__SECRET_ACCESS_KEY=your-secret-access-key
